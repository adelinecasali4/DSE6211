results <- list()
for (i in 1:nrow(parameterGrid)) {
model <- create_model(parameterGrid$units[i], parameterGrid$activation[i], parameterGrid$learning_rate[i])
history <- fit(model,
x = reduced_training_features,
y = training_labels,
epochs = 10,
batch_size = 512,
validation_split = 0.33)
results[[i]] <- list(model = model, history = history)
}
# Evaluate results and choose the best model
best_accuracy <- 0
best_model <- NULL
for (i in 1:length(results)) {
accuracy <- max(results[[i]]$history$metrics$val_accuracy)
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- results[[i]]$model
}
}
summary(best_model)
str(best_model)
activation_functions <- lapply(best_model$layers, `[[`, "activation")
print(activation_functions)
# Use the best model for predictions
predictions <- predict(best_model, reduced_test_features)
test_set$p_prob <- predictions[, 1]
pca_predicted_class <- ifelse(predictions[, 1] >= 0.5, 1, 0)
pca_accuracy <- mean(pca_predicted_class == test_labels)
pca_accuracy
# Making predictions and calculating fpr and tpr rates at 0.5 threshold
over_threshold <- test_set[test_set$p_prob >= 0.5, ]
fpr <- sum(over_threshold$booking_status==0)/sum(test_set$booking_status==0)
fpr
tpr <- sum(over_threshold$booking_status==1)/sum(test_set$booking_status==1)
tpr
# Plotting ROC curve
roc_data <- data.frame(threshold = seq(1, 0, -0.01), fpr = 0, tpr = 0)
for (i in roc_data$threshold) {
over_threshold <- test_set[test_set$p_prob >= i, ]
fpr <- sum(over_threshold$booking_status==0)/sum(test_set$booking_status==0)
roc_data[roc_data$threshold==i, "fpr"] <- fpr
tpr <- sum(over_threshold$booking_status==1)/sum(test_set$booking_status==1)
roc_data[roc_data$threshold==i, "tpr"] <- tpr
}
ggplot() +
geom_line(data = roc_data,
aes(x = fpr, y = tpr, color = threshold), linewidth = 2) +
scale_color_gradientn(colors = rainbow(3)) +
geom_abline(intercept = 0, slope = 1, lty = 2) +
geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = fpr, y = tpr)) +
geom_text(data = roc_data[seq(1, 101, 10), ],
aes(x = fpr, y = tpr, label = threshold, hjust = 1.2, vjust = -0.2))
# Calculating the AUC
pca_auc <- auc(x = roc_data$fpr, y = roc_data$tpr, type = "spline")
pca_auc
# Creating a calibration curve
in_interval <- test_set[test_set$p_prob >= 0.7 & test_set$p_prob <= 0.8, ]
nrow(in_interval[in_interval$booking_status==1, ])/nrow(in_interval)
calibration_data <- data.frame(bin_midpoint=seq(0.05,0.95,0.1),
observed_event_percentage=0)
for (i in seq(0.05,0.95,0.1)) {
in_interval <- test_set[test_set$p_prob >= (i-0.05) & test_set$p_prob <= (i+0.05), ]
oep <- nrow(in_interval[in_interval$booking_status==1, ])/nrow(in_interval)
calibration_data[calibration_data$bin_midpoint==i, "observed_event_percentage"] <- oep
}
ggplot(data = calibration_data, aes(x = bin_midpoint, y = observed_event_percentage)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, lty = 2) +
geom_point(size = 2) +
geom_text(aes(label = bin_midpoint), hjust = 0.75, vjust = -0.5)
# Table with models and relative accuracies
classification_overview <- data.frame(
Method = c("Logistic Regression", "kNN (k = 3)", "Random Forest", "Neural Network", "Neural Network with PCA"),
Accuracy = c("80.38%", "84.26%", "88.37%", "82.91%", "72.79%")
)
classification_table <- kable(classification_overview, "markdown") %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, bold = TRUE)
classification_table
# Table with models and relative accuracies
classification_overview <- data.frame(
Method = c("Logistic Regression", "kNN (k = 3)", "Random Forest", "Simple Neural Network", "Complex Neural Network", "Neural Network with PCA"),
Accuracy = c("80.38%", "84.23%", "88.37%", "80.02%", "84.10%", "72.11%")
)
classification_table <- kable(classification_overview, "markdown") %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, bold = TRUE)
classification_table
# Table with models and relative accuracies
classification_overview <- data.frame(
Method = c("Logistic Regression", "kNN (k = 3)", "Random Forest", "Simple Neural Network", "Complex Neural Network", "Neural Network with PCA"),
Accuracy = c("80.38%", "84.23%", "88.37%", "80.02%", "84.10%", "72.11%")
)
classification_table <- kable(classification_overview, "markdown") %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, bold = TRUE)
library(dplyr)
# Table with models and relative accuracies
classification_overview <- data.frame(
Method = c("Logistic Regression", "kNN (k = 3)", "Random Forest", "Simple Neural Network", "Complex Neural Network", "Neural Network with PCA"),
Accuracy = c("80.38%", "84.23%", "88.37%", "80.02%", "84.10%", "72.11%")
)
classification_table <- kable(classification_overview, "markdown") %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, bold = TRUE)
library(dplyr)
library(kableExtra)
# Table with models and relative accuracies
classification_overview <- data.frame(
Method = c("Logistic Regression", "kNN (k = 3)", "Random Forest", "Simple Neural Network", "Complex Neural Network", "Neural Network with PCA"),
Accuracy = c("80.38%", "84.23%", "88.37%", "80.02%", "84.10%", "72.11%")
)
classification_table <- kable(classification_overview, "markdown") %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, bold = TRUE)
classification_table
install.packages("NbClust")
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
cat("Cluster Sizes:\n")
table(km_clusters$cluster)
cat("\nCluster Centers:\n")
print(km_clusters$centers)
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means
## set the random number seed to obtain reproducible results
set.seed(1234)
## load any libraries that will be used
library(NbClust)
## view the data
data(wine, package = "rattle")
install.packages("rattke")
install.packages("rattle")
## set the random number seed to obtain reproducible results
set.seed(1234)
## load any libraries that will be used
library(NbClust)
## view the data
data(wine, package = "rattle")
head(wine)
## exclude the first column, since it contains the actual wine type
## we are going to try to identify the wine types using k-means clustering
## scale each of the variables to have a mean of zero and a standard deviation of one
wine_scaled <- scale(wine[, -1])
head(wine_scaled)
## use the NBClust function to search for the best number of clusters (between 2 and 15) for the kmeans algorithm
nc <- NbClust(wine_scaled, min.nc = 2, max.nc = 15, method = "kmeans")
barplot(table(nc$Best.n[1, ]), xlab = "Number of Clusters", ylab = "Number of Criteria",
main = "Number of Clusters Chosen by 26 Criteria")
fit.km <- kmeans(wine_scaled, centers = 3, nstart = 25)
## print the cluster sizes and cluster centers in a centroid table
fit.km$size
fit.km$centers
## create a data frame that contains the original data, as well as the cluster number for each observation
cluster_number <- data.frame(cluster_number = fit.km$cluster)
wine_clusters <- cbind(wine, cluster_number)
## explore the clusters to see if we can classify them based on the mean value for each variable within each cluster
aggregate(wine_clusters[, -1], by=list(wine_clusters$cluster_number), mean)
## compare the actual wine types with the clusters we found in the data - the clusters look very good!
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
View(training_features)
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
cat("Cluster Sizes:\n")
table(km_clusters$cluster)
cat("\nCluster Centers:\n")
print(km_clusters$centers)
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means
install.packages("clue")
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
head(test_cluster)
head(training_features)
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
head(test_cluster)
head(test_features)
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
head(test_features)
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
cat("Cluster Sizes:\n")
table(km_clusters$cluster)
cat("\nCluster Centers:\n")
print(km_clusters$centers)
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
# View the output
head(test_features)
# Load packages
library(dplyr)
library(caret)
library(NbClust)
# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
cat("Cluster Sizes:\n")
table(km_clusters$cluster)
cat("\nCluster Centers:\n")
print(km_clusters$centers)
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means
library(clue)
# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)
# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)
# View the output
head(test_features)
