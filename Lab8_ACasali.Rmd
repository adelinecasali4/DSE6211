---
title: "Lab 8"
author: "Adeline Casali"
date: "2024-03-07"
output: word_document
---

K-Means Clustering
```{r}
# Load packages
library(dplyr)
library(caret)
library(NbClust)

# Data preprocessing
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
                                    p = 0.75,
                                    list = FALSE,
                                    times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
  group_by(soil_type) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  select(soil_type) %>%
  top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
                                 training_set$soil_type,
                                 "other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
                            training_set[, c("wilderness_area", "soil_type")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
                               training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
                             test_set$soil_type,
                             "other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
                               center = apply(training_set[, -c(11:13)], 2, mean),
                               scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
                           dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
                         dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
                       dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
                     dim = c(nrow(test_set)))

# Performing k-means clustering
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)], 
              min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
```

1) When using k-means clustering, it's important to scale and center each variable to have a mean of 0 and a standard deviation of 1. If you don't do this, variables with larger magnitudes will have a stronger influence over the distance calculations, which are the backbone of the k-means clustering algorithm. 

2) Below are the cluster sizes and centers. 
```{r}
cat("Cluster Sizes:\n")
table(km_clusters$cluster)

cat("\nCluster Centers:\n")
print(km_clusters$centers)
```

3) Below are the means of each variable within each cluster. 
```{r}
cluster_means <- aggregate(. ~ cluster_number, data = training_features, FUN = mean)
cluster_means
```

4) Below I've assigned the observations in test_features to the clusters found above. 
```{r}
library(clue)

# Predict clusters for test_features
test_cluster <- cl_predict(km_clusters, newdata = test_features)

# Add the predicted cluster numbers to test_features
test_features <- cbind(test_features, cluster_number = test_cluster)

# View the output
head(test_features)
```

























