---
title: "Lab 4"
author: "Adeline Casali"
date: "2024-02-09"
output: word_document
---

Data Pre-Processing and Building the Model
```{r}
# Load packages and data
library(dplyr)
library(caret)
data <- read.csv("lab_4_data.csv")

# Create testing and training sets
training_ind <- createDataPartition(data$lodgepole_pine, 
                                    p = 0.75, 
                                    list = FALSE, 
                                    times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]

# Assessing, grouping, and factoring categorical variables
unique(training_set$wilderness_area)
unique(training_set$soil_type)

top_20_soil_types <- training_set %>% 
  group_by(soil_type) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>%
  select(soil_type) %>% 
  top_n(20)

training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type, 
                                 training_set$soil_type, 
                                 "other")

training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)

class(training_set$wilderness_area)
class(training_set$soil_type)

levels(training_set$wilderness_area)
levels(training_set$soil_type)

# One-hot encoding the training set
onehot_encoder <- dummyVars(~ wilderness_area + soil_type, 
                            training_set[, c("wilderness_area", "soil_type")], 
                            levelsOnly = TRUE, 
                            fullRank = TRUE)

onehot_enc_training <- predict(onehot_encoder, 
                               training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)

# One-hot encoding the test set
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type, 
                             test_set$soil_type, 
                             "other")

test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)

onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)

# Scaling test and training sets
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)], 
                               center = apply(training_set[, -c(11:13)], 2, mean), 
                               scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])

# Convert data sets to tensors
training_features <- array(data = unlist(training_set[, -c(11:13)]), 
                           dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]), 
                         dim = c(nrow(training_set)))

test_features <- array(data = unlist(test_set[, -c(11:13)]), 
                       dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]), 
                     dim = c(nrow(test_set)))

# Loading keras and tensorflow libraries
library(reticulate)
library(tensorflow)
library(keras)

# Building the model
model <- keras_model_sequential(list(
  layer_dense(units = 20, activation = "relu"), 
  layer_dense(units = 10, activation = "relu"), 
  layer_dense(units = 1, activation = "sigmoid")
))
compile(model, 
        optimizer = "rmsprop", 
        loss = "binary_crossentropy", 
        metrics = "accuracy")

# Training the model
history <- fit(model, training_features, training_labels, 
               epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history)

# Using the model to make predictions
predictions <- predict(model, test_features)
head(predictions, 10)
predicted_class <- (predictions[, 1] >= 0.5) * 1
head(predicted_class, 10)
```

1) Below are the loss and accuracy curves obtained from running the code above. 
```{r}
plot(history)
```

2) Below are the loss and accuracy curves obtained from running the code with 50 units for the first hidden layer and 25 units for the second hidden layer. 
```{r}
model1 <- keras_model_sequential(list(
  layer_dense(units = 50, activation = "relu"), 
  layer_dense(units = 25, activation = "relu"), 
  layer_dense(units = 1, activation = "sigmoid")
))
compile(model1, 
        optimizer = "rmsprop", 
        loss = "binary_crossentropy", 
        metrics = "accuracy")
history1 <- fit(model1, training_features, training_labels, 
               epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history1)
```

3) Based on the curves produced above, the original model with fewer nodes seems to have better performance. Although they both have extremely similar values for both loss and accuracy, the second model displays a high level of variability, particularly regarding the validation loss and accuracy curves. This could be a sign of overfitting or instability. The first model, on the other hand, is very consistent across all curves while maintaining similar overall values. 

4) Although they are very similar, the accuracy for the second model (with more units) is slightly better than that of the original model, at 71.68% for the first model and 72.69% for the second model. 
```{r}
results <- model %>% evaluate(test_features, test_labels)
results
results1 <- model1 %>% evaluate(test_features, test_labels)
results1
```













